# -*- coding: utf-8 -*-
"""live_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I-ee7pn_thBs56ZJP_I0SNrfeX79rV43
"""

import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
from IPython.display import display
import os

def scrape_kompas_page_1():
    """
    Scrapes the first page of Kompas tag "bencana" for articles.
    Returns a list of scraped articles.
    """
    # Base URL
    url = "https://www.kompas.com/tag/bencana?page=1"

    # Header untuk request
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }

    # Pola kategori
    category_patterns = {
        "Banjir": r"\bbanjir\b|\bbanjir bandang\b",
        "Gempa Bumi": r"\bgempa\b|\bgempa bumi\b",
        "Tanah Longsor": r"\btanah longsor\b|\blongsor\b",
        "Gunung Meletus": r"\bgunung meletus\b|\berupsi\b",
        "Tsunami": r"\btsunami\b",
        "Puting Beliung": r"\bputing beliung\b|\bbadai\b",
        "Kekeringan": r"\bkekeringan\b|\bkemarau panjang\b",
        "Cuaca Ekstrem": r"\bcuaca ekstrem\b|\bhujan es\b|\bgelombang panas\b",
        "Gelombang Ekstrem": r"\bgelombang ekstrem\b|\bgemuruh laut\b",
        "Kebakaran Hutan": r"\bkebakaran hutan\b|\bapi di hutan\b|\bkarhutla\b"
    }

    # Gabungkan semua pola menjadi satu regex
    keyword_pattern = re.compile("|".join(category_patterns.values()), re.IGNORECASE)

    # Request halaman
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Gagal mengakses halaman: {e}")
        return []

    soup = BeautifulSoup(response.content, 'html.parser')

    # Ambil artikel pada halaman
    articles = soup.find_all('div', class_='article__list__title')
    scraped_data = []

    for article in articles:
        try:
            # Judul dan URL
            title = article.find('h3', class_='article__title').get_text(strip=True)
            link = article.find('a', class_='article__link')['href']

            # Waktu
            date = article.find_next_sibling('div', class_='article__list__info').find('div', class_='article__date').get_text(strip=True)

            # Request konten artikel
            article_response = requests.get(link, headers=headers)
            if article_response.status_code != 200:
                print(f"Gagal mengakses artikel: {link}")
                continue

            article_soup = BeautifulSoup(article_response.content, 'html.parser')

            # Ambil semua paragraf sesuai aturan
            paragraphs = []
            for p in article_soup.find_all('p'):
                # Abaikan <p> dalam <div> class="footerCopyright"
                if p.find_parent('div', class_='footerCopyright'):
                    continue
                # Tambahkan teks hanya jika <p> tanpa class
                if not p.has_attr('class'):
                    text_parts = []
                    for elem in p.contents:
                        if isinstance(elem, str):
                            text_parts.append(elem.strip())
                        elif elem.name == 'a' and 'inner-link-tag' in elem.get('class', []):
                            text_parts.append(' ' + elem.get_text(strip=True) + ' ')  # Tambahkan spasi
                    text = ''.join(text_parts)
                    paragraphs.append(text)

            content = ' '.join(paragraphs)
            content = re.sub(r'^(Tim Redaksi -|Editor -)\s*', '', content)

            # Filter hanya artikel yang sesuai pola kategori
            if keyword_pattern.search(title) or keyword_pattern.search(content):
                # Tentukan kategori berdasarkan pola
                category = "Lainnya"
                for cat, pattern in category_patterns.items():
                    if re.search(pattern, content, re.IGNORECASE) or re.search(pattern, title, re.IGNORECASE):
                        category = cat
                        break

                # Tambahkan ke data
                scraped_data.append({
                    'URL': link,
                    'Date': date,
                    'Title': title,
                    'Content': content,
                    'Category': category
                })
        except Exception as e:
            print(f"Error pada artikel: {e}")

    return scraped_data

def run_scraping_interval():
    """
    Runs the Kompas scraping every 5 minutes and updates a single Excel file.
    """
    file_name = "kompas_bencana.xlsx"

    while True:
        print("Starting scraping process...")
        scraped_articles = scrape_kompas_page_1()

        if scraped_articles:
            # Cek apakah file sudah ada
            if os.path.exists(file_name):
                df_existing = pd.read_excel(file_name)
                existing_urls = set(df_existing['URL'])
            else:
                df_existing = pd.DataFrame(columns=["URL", "Date", "Title", "Content", "Category"])
                existing_urls = set()

            # Filter hanya artikel baru
            new_articles = [article for article in scraped_articles if article['URL'] not in existing_urls]
            if new_articles:
                print(f"Found {len(new_articles)} new articles. Saving to {file_name}...")
                df_new = pd.DataFrame(new_articles)
                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                df_combined.to_excel(file_name, index=False)
                print(f"File updated with new articles.")
            else:
                print("No new articles found.")
        else:
            print("No articles found in the scraping process.")

        # Tampilkan data terbaru dalam tabel interaktif menggunakan Jupyter Notebook
        print("Displaying the latest scraped articles:")
        df_sorted = pd.read_excel(file_name).sort_values(by="Date", ascending=False)
        display(df_sorted.head())  # Menampilkan tabel di Jupyter Notebook

        # Tunggu 5 menit sebelum scraping berikutnya
        print("Waiting for 5 minutes before the next run...")
        time.sleep(60)

# Jalankan scraping otomatis
#run_scraping_interval()

import requests
import re
import pandas as pd
from bs4 import BeautifulSoup
import time
import os

def scrape_detik_articles_with_categories(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.0.0"
    }

    # Definisikan pola untuk setiap kategori
    category_patterns = {
        "Banjir": r"\bbanjir\b|\bbanjir bandang\b",
        "Gempa Bumi": r"\bgempa\b|\bgempa bumi\b",
        "Tanah Longsor": r"\btanah longsor\b|\blongsor\b",
        "Gunung Meletus": r"\bgunung meletus\b|\berupsi\b",
        "Tsunami": r"\btsunami\b",
        "Puting Beliung": r"\bputing beliung\b|\bbadai\b|\bangin kencang\b",
        "Kekeringan": r"\bkekeringan\b|\bkemarau panjang\b",
        "Cuaca Ekstrem": r"\bcuaca ekstrem\b|\bhujan es\b|\bgelombang panas\b",
        "Gelombang Ekstrem": r"\bgelombang ekstrem\b|\bgemuruh laut\b",
        "Kebakaran Hutan": r"\bkebakaran hutan\b|\bapi di hutan\b|\bkarhutla\b"
    }

    # Gabungkan semua pola kategori menjadi satu pola regex
    keyword_pattern = re.compile("|".join(category_patterns.values()), re.IGNORECASE)

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching the page: {e}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.find_all("article")
    if not articles:
        print("No articles found.")
        return []

    news_list = []
    for article in articles:
        # Ambil link artikel
        link_tag = article.find("a", href=True)
        link = link_tag['href'] if link_tag else None

        # Abaikan URL yang mengandung "foto-news" atau "foto"
        if "foto-news" in link or "/foto/" in link:
            print(f"Skipping foto-related URL: {link}")
            continue

        # Ambil kategori dan tanggal
        date_tag = article.find("span", class_="date")
        date = date_tag.text.strip() if date_tag else None

        title_tag = article.find("h2", class_="title")
        title = title_tag.text.strip() if title_tag else "No title"

        # Scrap isi berita
        try:
            article_response = requests.get(link, headers=headers)
            article_response.raise_for_status()
            article_soup = BeautifulSoup(article_response.text, "html.parser")

            # Skip artikel jika mengandung "Video News"
            video_news_tag = article_soup.find("h2", class_="detail__subtitle", string="Video News")
            if video_news_tag:
                print(f"Skipping Video News article: {link}")
                continue

            # Hapus elemen "parallaxindetail scrollpage"
            for parallax_element in article_soup.find_all("div", class_="parallaxindetail scrollpage"):
                parallax_element.decompose()

            # Hapus elemen "ADVERTISEMENT" dan "SCROLL TO CONTINUE WITH CONTENT"
            for ad_element in article_soup.find_all("span", class_="para_caption", string="ADVERTISEMENT"):
                ad_element.decompose()
            for scroll_element in article_soup.find_all("p", class_="para_caption", string="SCROLL TO CONTINUE WITH CONTENT"):
                scroll_element.decompose()

            # Ambil teks dari elemen <p> dan gabungkan menjadi satu paragraf
            paragraphs = article_soup.find_all("p")
            content = " ".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            content = re.sub(r'\s+', ' ', content).strip()  # Normalisasi spasi
        except requests.exceptions.RequestException as e:
            print(f"Error fetching article content from {link}: {e}")
            content = "Failed to fetch content."

        # Filter artikel berdasarkan kata kunci di judul dan isi
        if keyword_pattern.search(title) or keyword_pattern.search(content):
            # Tentukan kategori
            category = "Tidak Dikategorikan"
            for cat, pattern in category_patterns.items():
                if re.search(pattern, title, re.IGNORECASE) or re.search(pattern, content, re.IGNORECASE):
                    category = cat
                    break

            # Simpan data artikel
            news_list.append({
                "URL": link,
                "Date": date,
                "Title": title,
                "Content": content,
                "Category": category
            })

    return news_list

def run_scraping_interval():
    base_url = "https://www.detik.com/tag/bencana-alam/?sortby=time&page=1"
    file_name = "detik_bencana.xlsx"

    while True:
        print("Starting scraping process...")
        articles = scrape_detik_articles_with_categories(base_url)

        if articles:
            if os.path.exists(file_name):
                df_existing = pd.read_excel(file_name)
                existing_urls = set(df_existing['URL'])
            else:
                df_existing = pd.DataFrame(columns=["URL", "Date", "Title", "Content", "Category"])
                existing_urls = set()

            new_articles = [article for article in articles if article['URL'] not in existing_urls]
            if new_articles:
                print(f"Found {len(new_articles)} new articles. Saving to {file_name}...")
                df_new = pd.DataFrame(new_articles)
                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                df_combined.to_excel(file_name, index=False)
                print(f"File updated with new articles.")
            else:
                print("No new articles found.")
        else:
            print("No articles found in the scraping process.")

        print("Displaying the latest scraped articles:")
        df_sorted = pd.read_excel(file_name).sort_values(by="Date", ascending=False)

        # Menampilkan DataFrame menggunakan Jupyter Notebook
        from IPython.display import display
        display(df_sorted.head())

        # Tunggu 5 menit sebelum scraping berikutnya
        print("Waiting for 5 minutes before the next run...")
        time.sleep(60)  # 5 menit

#run_scraping_interval()

import requests
import re
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from IPython.display import display
import time
import os

def scrape_cnn_today():
    """
    Scrapes CNN Indonesia articles for today's date, with pagination support.
    Returns a list of scraped articles.
    """
    category_patterns = {
        "Banjir": r"\bbanjir\b|\bbanjir bandang\b",
        "Gempa Bumi": r"\bgempa\b|\bgempa bumi\b",
        "Tanah Longsor": r"\btanah longsor\b|\blongsor\b",
        "Gunung Meletus": r"\bgunung meletus\b|\berupsi\b",
        "Tsunami": r"\btsunami\b",
        "Puting Beliung": r"\bputing beliung\b|\bbadai\b|\bangin kencang\b",
        "Kekeringan": r"\bkekeringan\b|\bkemarau panjang\b",
        "Cuaca Ekstrem": r"\bcuaca ekstrem\b|\bhujan es\b|\bgelombang panas\b",
        "Gelombang Ekstrem": r"\bgelombang ekstrem\b|\bgemuruh laut\b",
        "Kebakaran Hutan": r"\bkebakaran hutan\b|\bapi di hutan\b|\bkarhutla\b"
    }

    keyword_pattern = re.compile("|".join(category_patterns.values()), re.IGNORECASE)
    today_date = datetime.now().strftime("%Y/%m/%d")
    base_url = f"https://www.cnnindonesia.com/peristiwa/indeks/18?&date={today_date}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.0.0"
    }

    news_list = []
    page = 1  # Mulai dari halaman pertama

    while True:  # Loop untuk pagination
        current_url = f"{base_url}&page={page}"  # Tambahkan parameter halaman ke URL
        print(f"Scraping page: {current_url}")
        try:
            response = requests.get(current_url, headers=headers)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"Error fetching page {page}: {e}")
            break

        soup = BeautifulSoup(response.text, "html.parser")

        # Cari semua elemen artikel di halaman
        articles = soup.find_all("article", class_="flex-grow")
        if not articles:  # Jika tidak ada artikel, hentikan pagination
            print(f"No articles found on page {page}. Stopping pagination.")
            break

        for article in articles:
            link_tag = article.find("a", href=True)
            link = link_tag['href'] if link_tag else None

            if link and link.startswith("http"):
                title = link_tag.text.strip() if link_tag else "No title"

                # Skip artikel jika tidak ada kata kunci di judul
                if not keyword_pattern.search(title):
                    print(f"Skipping article (no matching keywords in title): {title}")
                    continue

                print(f"Scraping article: {title}")

                try:
                    article_response = requests.get(link, headers=headers)
                    article_response.raise_for_status()
                    article_soup = BeautifulSoup(article_response.text, "html.parser")

                    date_element = article_soup.find("div", class_="text-cnn_grey text-sm mb-4")
                    date = date_element.text.strip() if date_element else "No date found"

                    # Selector untuk isi berita
                    content_element = article_soup.find("div", class_="detail-text text-cnn_black text-sm grow min-w-0")
                    if content_element:
                        # Hapus elemen <div class="paradetail"> (iklan) dari content_element
                        for ad_div in content_element.find_all("div", class_="paradetail"):
                            ad_div.decompose()

                        # Ambil semua elemen <p> setelah elemen iklan dihapus
                        content_paragraphs = content_element.find_all("p")

                        content = ""
                        for p in content_paragraphs:
                            p_text = ""

                            # Memastikan semua elemen di dalam <p> diproses, termasuk <span> dan <a>
                            for element in p.children:
                                if element.name == "span" or element.name == "a":  # Proses elemen <span> dan <a>
                                    p_text += " " + element.get_text(strip=True) + " "
                                elif element.name is None:  # Jika elemen adalah teks biasa
                                    p_text += element.strip() + " "

                            # Gabungkan teks yang ada di dalam <p> setelah ditambahkan spasi di antara elemen
                            content += p_text.strip() + " "  # Gabungkan semua teks dengan spasi antar elemen
                    else:
                        content = "No content found"

                    if not content or content == "No content found":
                        print(f"Skipping article due to empty content: {title}")
                        continue

                    if not keyword_pattern.search(content):
                        print(f"Skipping article (no matching keywords in content): {title}")
                        continue

                    category = "Tidak Dikategorikan"
                    for cat, pattern in category_patterns.items():
                        if re.search(pattern, title, re.IGNORECASE) or re.search(pattern, content, re.IGNORECASE):
                            category = cat
                            break

                    news_list.append({
                        "URL": link,
                        "Date": date,
                        "Title": title,
                        "Content": content,
                        "Category": category
                    })

                except requests.exceptions.RequestException as e:
                    print(f"Error fetching article {link}: {e}")
            else:
                print(f"Skipping invalid link: {link}")

        # Periksa apakah ada halaman berikutnya
        next_page = soup.find("a", class_="text-white bg-cnn_red inline-flex items-center justify-center w-[30px] h-[30px] rounded-md", href=True)
        if not next_page:
            print(f"No more pages after page {page}. Stopping pagination.")
            break

        page += 1  # Lanjutkan ke halaman berikutnya

    return news_list

def run_scraping_today():
    """
    Runs the CNN Indonesia scraping for today's date every 5 minutes and updates a single Excel file.
    """
    file_name = "cnn_today_with_pagination.xlsx"

    while True:
        print("Starting scraping process for today...")
        scraped_articles = scrape_cnn_today()

        if scraped_articles:
            if os.path.exists(file_name):
                df_existing = pd.read_excel(file_name)
                existing_urls = set(df_existing['URL'])
            else:
                df_existing = pd.DataFrame(columns=["URL", "Date", "Title", "Content", "Category"])
                existing_urls = set()

            new_articles = [article for article in scraped_articles if article['URL'] not in existing_urls]
            if new_articles:
                print(f"Found {len(new_articles)} new articles. Saving to {file_name}...")
                df_new = pd.DataFrame(new_articles)
                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                df_combined.to_excel(file_name, index=False)
                print(f"File updated with new articles.")
            else:
                print("No new articles found.")
        else:
            print("No articles found in the scraping process.")

        print("Displaying the latest scraped articles:")
        df_sorted = pd.read_excel(file_name).sort_values(by="Date", ascending=False)
        display(df_sorted.head())

        print("Waiting for 5 minutes before the next run...")
        time.sleep(60)

def scrape_liputan6_live():
    """
    Scrapes the latest articles from Liputan6 News Indeks for the current day.
    Returns a list of dictionaries containing article details.
    """
    from datetime import datetime

    base_url = "https://www.liputan6.com/news/indeks/{}/{}/{}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }
    category_patterns = {
        "Banjir": r"\bbanjir\b|\bbanjir bandang\b",
        "Gempa Bumi": r"\bgempa\b|\bgempa bumi\b",
        "Tanah Longsor": r"\btanah longsor\b|\blongsor\b",
        "Gunung Meletus": r"\bgunung meletus\b|\berupsi\b",
        "Tsunami": r"\btsunami\b",
        "Puting Beliung": r"\bputing beliung\b|\bbadai\b",
        "Kekeringan": r"\bkekeringan\b|\bkemarau panjang\b",
        "Cuaca Ekstrem": r"\bcuaca ekstrem\b|\bhujan es\b|\bgelombang panas\b",
        "Gelombang Ekstrem": r"\bgelombang ekstrem\b|\bgemuruh laut\b",
        "Kebakaran Hutan": r"\bkebakaran hutan\b|\bapi di hutan\b|\bkarhutla\b"
    }
    keyword_pattern = re.compile("|".join(category_patterns.values()), re.IGNORECASE)

    # Target tanggal hari ini
    current_date = datetime.now()
    year, month, day = current_date.strftime("%Y"), current_date.strftime("%m"), current_date.strftime("%d")
    data = []
    page = 1

    while True:
        url = f"{base_url.format(year, month, day)}?page={page}"
        print(f"Scraping URL: {url}")

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            print(f"Failed to fetch URL: {url}, Status Code: {response.status_code}")
            break

        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.find_all('span', class_='articles--rows--item__title-link-text')
        if not articles:
            print(f"No more articles on page {page}.")
            break

        for article in articles:
            try:
                title = article.get_text(strip=True)
                article_url = article.find_parent('a')['href']

                # Fetch article content
                article_response = requests.get(article_url, headers=headers)
                if article_response.status_code != 200:
                    continue

                article_soup = BeautifulSoup(article_response.content, 'html.parser')
                full_title = article_soup.find('h1', class_='read-page--header--title entry-title').get_text(strip=True)
                date_element = article_soup.find('time', class_='read-page--header--author__modified-time updated')
                date = date_element['datetime'] if date_element else current_date.strftime('%Y-%m-%d')

                content_elements = article_soup.find_all('div', class_='article-content-body__item-content')
                content = ' '.join([
                    p.get_text(strip=True) for div in content_elements for p in div.find_all('p')
                    if not p.get_text(strip=True).startswith("Baca Juga")
                ])

                # Filter only articles with matching keywords
                if not (keyword_pattern.search(title) or keyword_pattern.search(content)):
                    continue

                # Determine category
                category = "Lainnya"
                for cat, pattern in category_patterns.items():
                    if re.search(pattern, title, re.IGNORECASE) or re.search(pattern, content, re.IGNORECASE):
                        category = cat
                        break

                # Add to data
                data.append({
                    "URL": article_url,
                    "Date": date,
                    "Title": full_title,
                    "Content": content,
                    "Category": category
                })

            except Exception as e:
                print(f"Error processing article: {e}")

        page += 1

    return data
def scrape_all_sources():
    """
    Menggabungkan hasil scraping dari berbagai sumber berita.
    """
    try:
        print("Scraping Kompas...")
        kompas_data = scrape_kompas_page_1()

        print("Scraping Detik...")
        detik_data = scrape_detik_articles_with_categories("https://www.detik.com/tag/bencana-alam/?sortby=time&page=1")

        print("Scraping CNN...")
        cnn_data = scrape_cnn_today()

        print("Scraping Liputan6...")
        liputan6_data = scrape_liputan6_live()

        # Gabungkan semua data
        all_data = kompas_data + detik_data + cnn_data + liputan6_data
        print(f"Total artikel yang di-scrape: {len(all_data)}")
        return all_data
    except Exception as e:
        print(f"Error saat scraping: {e}")
        return []
    
# Jalankan scraping otomatis untuk tanggal hari ini
#run_scraping_today()